{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports\n",
   "id": "a90848538d5601d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:38:22.121173Z",
     "start_time": "2025-05-22T19:38:15.791077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n"
   ],
   "id": "482f994919130b54",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utility Functions",
   "id": "6bbf5a876551ad29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:37:34.031726Z",
     "start_time": "2025-05-22T19:37:34.009550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_rows_by_column_value(df, column_name, value_to_remove):\n",
    "    \"\"\"\n",
    "    Remove all rows from a DataFrame where a specific column matches a given value.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column to filter.\n",
    "        value_to_remove (str): The value to remove from the column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified rows removed.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified column is not in the DataFrame.\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "\n",
    "    filtered_df = df[df[column_name] != value_to_remove].copy()\n",
    "    return filtered_df\n",
    "\n",
    "def remove_duplicate_rows(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame without duplicate rows.\n",
    "    \"\"\"\n",
    "    cleaned_df = df.drop_duplicates().copy()\n",
    "    return cleaned_df\n",
    "\n",
    "def sample_dataframe(df, percentage, random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly sample a percentage of rows from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        percentage (float): Percentage of rows to sample (between 0 and 100).\n",
    "        random_state (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A sampled subset of the original DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If percentage is not between 0 and 100.\n",
    "    \"\"\"\n",
    "    if not (0 < percentage <= 100):\n",
    "        raise ValueError(\"Percentage must be between 0 and 100.\")\n",
    "\n",
    "    frac = percentage / 100.0\n",
    "    sampled_df = df.sample(frac=frac, random_state=random_state).copy()\n",
    "    return sampled_df\n",
    "\n",
    "def stratified_sample_by_column(df, column, percentage, random_state=None):\n",
    "    \"\"\"\n",
    "    Reduce a DataFrame to a given percentage while preserving the distribution\n",
    "    of values in a specific column (stratified sampling).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): Column name to preserve distribution (e.g., \"Cuisine\").\n",
    "        percentage (float): Percentage of total rows to keep (0 < percentage <= 100).\n",
    "        random_state (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new stratified-sampled DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If percentage is not between 0 and 100.\n",
    "        KeyError: If the specified column does not exist.\n",
    "    \"\"\"\n",
    "    if not (0 < percentage <= 100):\n",
    "        raise ValueError(\"Percentage must be between 0 and 100.\")\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    sampled_dfs = []\n",
    "    frac = percentage / 100.0\n",
    "\n",
    "    for group_value, group_df in df.groupby(column):\n",
    "        group_sample = group_df.sample(frac=frac, random_state=random_state)\n",
    "        sampled_dfs.append(group_sample)\n",
    "\n",
    "    result_df = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "    return result_df\n",
    "\n",
    "def calculate_network_descriptors(G):\n",
    "    \"\"\"\n",
    "    Compute key structural and macroscopic descriptors for the given network.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following metrics:\n",
    "            - 'nodes': Number of nodes in the network.\n",
    "            - 'edges': Number of edges in the network.\n",
    "            - 'self_loops': Number of self-loops.\n",
    "            - 'multi_edges': Number of multi-edges.\n",
    "            - 'degree_min': Minimum node degree.\n",
    "            - 'degree_max': Maximum node degree.\n",
    "            - 'degree_avg': Average node degree.\n",
    "            - 'clustering': Average clustering coefficient.\n",
    "            - 'assortativity': Degree assortativity coefficient.\n",
    "            - 'avg_path_length': Average shortest path length (largest connected component).\n",
    "            - 'diameter': Diameter of the largest connected component.\n",
    "    \"\"\"\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "\n",
    "    self_loops = nx.number_of_selfloops(G)\n",
    "    multi_edges = sum(1 for edge in G.edges() if G.number_of_edges(edge[0], edge[1]) > 1)\n",
    "\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    g_largest_cc = G.subgraph(largest_cc)\n",
    "\n",
    "    return {\n",
    "        'nodes': n_nodes,\n",
    "        'edges': n_edges,\n",
    "        'self_loops': self_loops,\n",
    "        'multi_edges': multi_edges,\n",
    "        'degree_min': np.min(degrees),\n",
    "        'degree_max': np.max(degrees),\n",
    "        'degree_avg': np.mean(degrees),\n",
    "        'clustering': nx.average_clustering(G),\n",
    "        'assortativity': nx.degree_assortativity_coefficient(G),\n",
    "        'avg_path_length': nx.average_shortest_path_length(g_largest_cc),\n",
    "        'diameter': nx.diameter(g_largest_cc),\n",
    "    }\n",
    "\n",
    "def calculate_centralities(G):\n",
    "    \"\"\"\n",
    "    Compute node centrality measures for a single network.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        - dict: A dictionary with centrality measures:\n",
    "            - 'betweenness': Betweenness centrality for each node.\n",
    "            - 'degree': Degree centrality for each node.\n",
    "            - 'eigenvector': Eigenvector centrality for each node.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'betweenness': nx.betweenness_centrality(G),\n",
    "        'degree': dict(G.degree()),\n",
    "        'eigenvector': nx.eigenvector_centrality(G)\n",
    "    }\n",
    "\n",
    "def plot_degree_distribution(degree_sequence, net_name, save=True, show=False, out_dir='report'):\n",
    "    \"\"\"\n",
    "    Plot degree distribution in both linear and logarithmic scales.\n",
    "\n",
    "    Parameters:\n",
    "        degree_sequence (list): List of node degrees.\n",
    "        net_name (str): Network name for title and filename.\n",
    "        save (bool): If True, saves the plot as PNG in the output directory.\n",
    "        show (bool): If True, displays the plot.\n",
    "        out_dir (str): Output directory for saving the plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    degree_counts = Counter(degree_sequence)\n",
    "    min_degree = min(degree_sequence)\n",
    "    max_degree = max(degree_sequence)\n",
    "\n",
    "    degree = list(range(min_degree, max_degree + 1))\n",
    "    degree_count = [degree_counts.get(x, 0) for x in degree]\n",
    "\n",
    "    # Linear scale\n",
    "    ax[0].scatter(degree, degree_count, color='blue', label='data')\n",
    "    ax[0].set_xlabel('$k$', fontsize=14)\n",
    "    ax[0].set_ylabel('$P(k)$', fontsize=14)\n",
    "    ax[0].set_title('Linear scale', fontsize=14)\n",
    "    ax[0].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    # Log-log scale\n",
    "    ax[1].scatter(degree, degree_count, color='red', label='data')\n",
    "    ax[1].set_xlabel('$k$', fontsize=14)\n",
    "    ax[1].set_ylabel('$P(k)$', fontsize=14)\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_title('Logarithmic scale', fontsize=14)\n",
    "    ax[1].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    plt.suptitle(f'Degree Distribution: {net_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        file_path = os.path.join(out_dir, f\"{net_name}_degree_distribution.png\")\n",
    "        plt.savefig(file_path, bbox_inches='tight')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "def add_bipartite_nodes_from_dataframe(G, df, column_id, bipartite, prefix):\n",
    "    \"\"\"\n",
    "    Add nodes to a graph from a DataFrame for a bipartite graph construction.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The graph to which nodes are added.\n",
    "        df (pd.DataFrame): The DataFrame containing node data.\n",
    "        column_id (str): The name of the column with unique node identifiers.\n",
    "        bipartite (int): The bipartite set this node belongs to (0 or 1).\n",
    "        prefix (str): Prefix to prepend to the node ID (e.g., 'R' for recipes, 'I' for ingredients).\n",
    "\n",
    "    Returns:\n",
    "        None (the graph is modified in-place).\n",
    "    \"\"\"\n",
    "    if column_id not in df.columns:\n",
    "        raise KeyError(f\"Column '{column_id}' not found in DataFrame.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        node_id = f\"{prefix}{row[column_id]}\"\n",
    "        attrs = row.drop(column_id).to_dict()\n",
    "        attrs['bipartite'] = bipartite\n",
    "        G.add_node(node_id, **attrs)\n",
    "\n",
    "def apply_greedy_community_detection(G, weight=None):\n",
    "    \"\"\"\n",
    "    Applies the Greedy Modularity algorithm to the given graph.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The graph on which to run community detection.\n",
    "        weight (str or None): Edge attribute to use as weight. If None, the graph is unweighted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sets, where each set contains the nodes in one community.\n",
    "    \"\"\"\n",
    "    return list(greedy_modularity_communities(G, weight=weight))\n",
    "\n",
    "\n",
    "def detect_communities_louvain(G):\n",
    "    \"\"\"\n",
    "    Apply Louvain algorithm to detect communities in G using NetworkX interface.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): Input graph.\n",
    "\n",
    "    Returns:\n",
    "        communities (list[set]): Detected communities.\n",
    "    \"\"\"\n",
    "    return list(nx.community.louvain_communities(G))\n",
    "\n",
    "def add_edges_from_dataframe(G, df, col1, col2, prefix1, prefix2):\n",
    "    \"\"\"\n",
    "    Add edges to a graph from a DataFrame using two columns and respective prefixes.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The graph to which edges are added.\n",
    "        df (pd.DataFrame): The DataFrame containing edge data.\n",
    "        col1 (str): Column name for the first node ID (e.g., \"Recipe ID\").\n",
    "        col2 (str): Column name for the second node ID (e.g., \"Entity ID\").\n",
    "        prefix1 (str): Prefix for nodes from col1 (e.g., \"R\").\n",
    "        prefix2 (str): Prefix for nodes from col2 (e.g., \"I\").\n",
    "\n",
    "    Returns:\n",
    "        None (the graph is modified in-place).\n",
    "    \"\"\"\n",
    "    if col1 not in df.columns or col2 not in df.columns:\n",
    "        raise KeyError(f\"Columns '{col1}' and/or '{col2}' not found in DataFrame.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        node1 = f\"{prefix1}{row[col1]}\"\n",
    "        node2 = f\"{prefix2}{row[col2]}\"\n",
    "        G.add_edge(node1, node2)"
   ],
   "id": "630cc9f2c43037f9",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
